{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Goals\n",
    "The goal of this notebook is to compare the outcomes of FHA and HOLC zones on home value, ownership, and segregation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "paper_cmap = {\"A\":\"#5FCE72\",\"B\":\"#0BC0ED\",\"C\":\"#FFD419\",\"D\":\"#FF4B19\"}\n",
    "paper_colors = LinearSegmentedColormap('paper_colors', paper_cmap)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "import glob\n",
    "import requests\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely.geometry.multipolygon import MultiPolygon\n",
    "from shapely import wkt\n",
    "\n",
    "import psycopg2  # (if it is postgres/postgis)\n",
    "conn = psycopg2.connect(database=\"postgres\", user=\"wenfeixu\", password=\"\",\n",
    "    host=\"localhost\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('postgresql://wenfeixu:@localhost:5432/postgres')\n",
    "\n",
    "from geoalchemy2 import Geometry, WKTElement\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from patsylearn import PatsyModel, PatsyTransformer\n",
    "\n",
    "import libpysal as ps\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats as st\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Get Data\n",
    "### 0.1 Get data from HOLC cities\n",
    "\n",
    "`chicago_historical` is the baseline dataset I'm working with which has the cross-walked census boundaries. The normalized data has been normalized by year, so they represent the number of standard deviations from the norm for that particulalr year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = gpd.read_file('data/chicago_census_historical/chicago_census_historical.shp')\n",
    "\n",
    "df_final = df_final.rename(columns={'asian': 'asian',\n",
    " 'asian_pe_1': 'asian_perc_norm',\n",
    " 'asian_perc': 'asian_perc',\n",
    " 'black': 'black',\n",
    " 'black_norm': 'black_norm',\n",
    " 'black_pe_1': 'black_perc_norm',\n",
    " 'black_perc': 'black_perc',\n",
    " 'college': 'college',\n",
    " 'college__1': 'college_perc_norm',\n",
    " 'college_pe': 'college_perc',\n",
    " 'gisjoin': 'gisjoin',\n",
    " 'gisjoin_19': 'gisjoin_1940',\n",
    " 'hispanic': 'hispanic',\n",
    " 'hispanic_1': 'hispanic_perc_norm',\n",
    " 'hispanic_p': 'hispanic_perc',\n",
    " 'holc_grade': 'holc_grade',\n",
    " 'homes': 'homes',\n",
    " 'ln_homeval': 'ln_homeval_norm',\n",
    " 'ln_income_': 'ln_income_norm',\n",
    " 'ln_media_1': 'ln_median_homevalue_adj',\n",
    " 'ln_median_': 'ln_median_income',\n",
    " 'map_id': 'map_id',\n",
    " 'median_h_1': 'median_homevalue_adj',\n",
    " 'median_hom': 'median_homevalue',\n",
    " 'median_i_1': 'median_income',\n",
    " 'median_inc': 'median_income_adj',\n",
    " 'other': 'other',\n",
    " 'other_perc': 'other_perc',\n",
    " 'owned_pe_1': 'owned_perc_norm',\n",
    " 'owned_perc': 'owned_perc',\n",
    " 'populati_1': 'population_density',\n",
    " 'populati_2': 'population_density_norm',\n",
    " 'populati_3': 'population_norm',\n",
    " 'population': 'population',\n",
    " 'unemploy_1': 'unemployed_perc',\n",
    " 'unemploy_2': 'unemployed_perc_norm',\n",
    " 'unemployed': 'unemployed',\n",
    " 'white': 'white',\n",
    " 'white_norm': 'white_norm',\n",
    " 'white_pe_1': 'white_perc_norm',\n",
    " 'white_perc': 'white_perc',\n",
    " 'year': 'year'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Feature creation\n",
    "Need to create some extra features, including for later propensity scoring to find similarity between 'before' tracts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['nonblack_perc'] = 1-df_final['black_perc']/df_final['population']\n",
    "\n",
    "df_final['perc_college_10plus'] = [1 if x>0.10  else 0 for x in df_final['college_perc']]\n",
    "\n",
    "\n",
    "# Need to create some extra features for the propensity scoring to increase similarity between 'before' tracts. \n",
    "df_final['perc_black_20plus'] = [1 if x>0.20  else 0 for x in df_final['black_perc']]\n",
    "df_final['perc_college_10plus'] = [1 if x>0.10  else 0 for x in df_final['college_perc']]\n",
    "\n",
    "# Create a point that represents roughly downtown is \n",
    "downtown = Point(-87.62,41.855)\n",
    "df_final['dist_downtown'] = df_final.distance(Point(-87.62,41.855))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create different treatment periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1 = df_final[df_final.year!=1950]\n",
    "df_final1.loc[df_final1.year<=1940,'period']='pre'\n",
    "df_final1.loc[(df_final1.year>1940)&\n",
    "                   (df_final1.year<=1980),'period']='post'\n",
    "\n",
    "df_final1.loc[(df_final1.year>1980),'period']='reversal'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Feature creation for Propensity Score Matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the D vs non-D FHA labels\n",
    "df_final1['D_fha'] = df_final1.apply(lambda x: 'D' if x['fha_grade']=='D' else 'not_D_fha',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Clean and standardize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decade_scaler(x):\n",
    "    scaler =preprocessing.MinMaxScaler()\n",
    "    return scaler.fit_transform(x)\n",
    "\n",
    "def stand_data(df,features,groups = ['map_id','year']):\n",
    "    for f in features: \n",
    "        df['{}_norm'.format(f)] = df[groups+features].groupby(groups).transform(lambda x: (x-x.mean())/x.std())[f]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tracts(df):\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    df = df[(df['ln_median_homevalue_adj']>0)&(df['owned_perc']>0)]\n",
    "    df=df.drop_duplicates(['gisjoin','year'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final1=clean_tracts(df_final1)\n",
    "\n",
    "## Already did this earlier\n",
    "# stand_features=['owned_perc','ln_median_homevalue_adj','white_perc','black_perc','unemployed_perc','asian_perc','hispanic_perc',\n",
    "#                'population_density','population','ppl_per_home','dist_downtown']\n",
    "# df_final = stand_data(df_final,stand_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Entropy and Information Theory Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_E(df,feature='black_prop'):\n",
    "    tot = df['population'].sum()\n",
    "    p=(df[feature]*df['population'])/tot+.00000001\n",
    "    p = p.sum()\n",
    "    E = -p*np.log2(p)-\\\n",
    "         (1-p)*np.log2(1-p)\n",
    "        \n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entropy(d): \n",
    "    \n",
    "    '''\n",
    "    Creates both the dissimiliarity measure and weighted entropy measure based on black \n",
    "    and non-black populations \n",
    "    '''\n",
    "    \n",
    "    d['e'] = 0\n",
    "    d['e_w'] = 0\n",
    "   \n",
    "    for y in d['year'].unique():\n",
    "        W = ps.weights.Queen.from_dataframe(d[d.year==y],geom_col='geometry',idVariable='gisjoin')\n",
    "        \n",
    "        #### Make sure the denominator is not zero\n",
    "        d.loc[d['year']==y,'e'] =\\\n",
    "        d.groupby('year').apply(lambda x: -x['black_perc'].values*np.log2(x['black_perc'].values+.000001)-\\\n",
    "                                                x['nonblack_perc'].values*np.log2(x['nonblack_perc'].values+.000001))[y]\n",
    "        W.transform = 'R'\n",
    "        \n",
    "        d_yr = d[d.year==y]\n",
    "        for gisjoin in d_yr[d_yr.year==y].gisjoin.values:\n",
    "\n",
    "\n",
    "            w_black_prop = np.average(list(d_yr[d_yr.gisjoin==gisjoin]['black_perc'].values)+\n",
    "                                      list(d_yr.loc[(d_yr.gisjoin.isin(W.neighbors[gisjoin])),'black_perc']),\n",
    "                                      weights=[1]+W.weights[gisjoin] )\n",
    "            w_nonblack_prop = np.average(list(d_yr[d_yr.gisjoin==gisjoin]['nonblack_perc'].values)+\n",
    "                                         list(d_yr.loc[(d_yr.gisjoin.isin(W.neighbors[gisjoin])),'nonblack_perc'].values),\n",
    "                                         weights=[1]+ W.weights[gisjoin])\n",
    "            try: \n",
    "                d.loc[(d['year']==y)&(d['gisjoin']==gisjoin),'e_w'] =-w_black_prop*np.log2(w_black_prop+.000001)-\\\n",
    "                                                w_nonblack_prop*np.log2(w_nonblack_prop+.000001)\n",
    "                \n",
    "            except (not W.neighbors[gisjoin]) or KeyError:\n",
    "                print(gisjoin)\n",
    "                d.loc[(d['year']==y)&(d['gisjoin']==gisjoin),'e_w'] = d.loc[(d['year']==y)&(d['gisjoin']==gisjoin),'e']\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2 = get_entropy(df_final1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prep for propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create a DF for PSM purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm = df_final2[df_final2.period=='pre']\n",
    "df_final_psm['fha_grade_psm'] = df_final_psm['fha_grade'].apply(lambda x: 'D' if x=='D' else 'not_D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm['black_perc_chng']= df_final_psm['black_perc'].diff()\n",
    "df_final_psm['white_perc_chng']= df_final_psm['white_perc'].diff()\n",
    "df_final_psm['population_density_chng_perc']= df_final_psm['population_density'].pct_change()\n",
    "df_final_psm['ln_median_homevalue_adj_chng_perc']= df_final_psm['ln_median_homevalue_adj'].diff()\n",
    "df_final_psm['owned_perc_chng']= df_final_psm['owned_perc'].diff()\n",
    "df_final_psm['population_chng_perc']= df_final_psm['population'].pct_change()\n",
    "df_final_psm['e_w_chng_perc']= df_final_psm['e_w'].pct_change()\n",
    "df_final_psm['e_chng']= df_final_psm['e_w'].diff()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2  Remove all public data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data = gpd.read_file('data/chicago_housing/chicago_publichousing_historic.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many tracts intersect with public data? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm[df_final_psm.gisjoin.isin(gpd.sjoin(df_final_psm,\n",
    "                                         public_data).gisjoin.unique())].groupby('fha_grade').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the tracts that don't have any public data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm = df_final_psm[~df_final_psm.gisjoin.isin(gpd.sjoin(df_final_psm,\n",
    "                                                                public_data).gisjoin.unique())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Pre-Treatment\n",
    "Select and average over pre-treatment period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm = df_final_psm[(df_final_psm.period=='pre')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm = df_final_psm.groupby(['period','gisjoin']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Join back with FHA and HOLC Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm = df_final2[df_final2.year==1940][['gisjoin','fha_grade','holc_grade','geometry']].merge(df_final_psm,on='gisjoin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Join with neighorhood boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighorhoods = gpd.read_file('data/chicago_neighborhoods/Neighborhoods_2012b.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tract_n_labels = gpd.overlay(df_final_psm,neighorhoods[['PRI_NEIGH', 'SEC_NEIGH', 'geometry']].to_crs({'init': 'epsg:4326'}))\n",
    "tract_n_labels['overlay_area']=tract_n_labels.geometry.area\n",
    "tract_n_labels = tract_n_labels.sort_values(['gisjoin','overlay_area'],ascending=False)\\\n",
    "                                .groupby('gisjoin')\\\n",
    "                                .first()\\\n",
    "                                .reset_index()\n",
    "            \n",
    "df_final_psm = df_final_psm.merge(tract_n_labels[['gisjoin','PRI_NEIGH','SEC_NEIGH']],on='gisjoin',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create a new DF that averages over periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_avg = df_final2[df_final2.year==1940][['gisjoin','fha_grade','D_fha','holc_grade','geometry']]\\\n",
    "                                                    .merge(df_final2.groupby(['gisjoin','period'])\\\n",
    "                                                                   .mean()\\\n",
    "                                                                   .reset_index(),\n",
    "                                                   on='gisjoin')\n",
    "df_final_avg1 = df_final_avg.merge(df_final_psm[['white_perc_chng','black_perc_chng','population_density_chng_perc',\n",
    "                                                 'population_chng_perc','ln_median_homevalue_adj_chng_perc',\n",
    "                                                 'owned_perc_chng','e_w_chng_perc','gisjoin']],on='gisjoin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_avg1_export = df_final_avg1.copy()\n",
    "df_final_avg1_export.to_postgis('df_final_avg', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Background Values and Charts for Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Test for spatial autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.moran import Moran\n",
    "\n",
    "for i in ['ln_median_homevalue_adj','owned_perc','e_w']:\n",
    "    print(i)\n",
    "    for y in df_final_avg1.period.unique():\n",
    "        W = ps.weights.Queen.from_dataframe(df_final_avg1[df_final_avg1.period==y].to_crs(epsg=3857),ids='gisjoin',geom_col='geometry')\n",
    "        mi = Moran(df_final_avg1[df_final_avg1.period==y][i].values, W, two_tailed=True)\n",
    "        print(\"%.3f\"%mi.I,\"%.5f\"%mi.p_norm, \"%.3f\"%mi.EI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Test for temporal autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sorting order to autocorrelation purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_avg1.loc[df_final_avg1.period=='pre','per_order'] = 1\n",
    "df_final_avg1.loc[df_final_avg1.period=='post','per_order'] = 2\n",
    "df_final_avg1.loc[df_final_avg1.period=='reversal','per_order'] = 3\n",
    "\n",
    "df_final_avg1 = df_final_avg1.sort_values(['gisjoin','per_order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "row = 0\n",
    "fig,ax = plt.subplots(3,4,figsize=(24,14))\n",
    "for i in ['ln_median_homevalue_adj','owned_perc','e_w']:\n",
    "    counter = 0\n",
    "    for grade in ['A','B','C','D']:\n",
    "        \n",
    "        plot_acf(df_final2[(df_final2['fha_grade']==grade)][i].fillna(0), \n",
    "                 lags=7,\n",
    "                 ax =ax[row][counter],\n",
    "                 title='Autocorr: Grade {} | {}'.format(grade,i))\n",
    "        ax[row][counter].set_xlabel('Years after 1930')\n",
    "        counter +=1\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "fig,ax = plt.subplots(3,4,figsize=(24,11))\n",
    "for i in ['ln_median_homevalue_adj','owned_perc','e_w']:\n",
    "    counter = 0\n",
    "    for grade in ['A','B','C','D']:\n",
    "        \n",
    "        plot_acf(df_final_avg1[(df_final_avg1.fha_grade==grade)][i].fillna(0), \n",
    "                 lags=2,\n",
    "                 ax =ax[row][counter],\n",
    "                 title='Autocorr: Grade {} | {}'.format(grade,i))\n",
    "        counter +=1\n",
    "    row+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Trends over Time\n",
    "\n",
    "This plot looks at the FHA labels and tracks them over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "\n",
    "summary_features = [ 'population','population_density','unemployed_perc',\\\n",
    "              'black_perc','white_perc','asian_perc','hispanic_perc',\n",
    "                 'college_perc','owned_perc','homes','median_homevalue_adj','e_w']\n",
    "\n",
    "df_final_plots = df_final2.replace(0,np.nan)\n",
    "f, axes = plt.subplots(4, 3, figsize=(14, 12), sharex=True)\n",
    "sns.despine(left=True)\n",
    "for i,each in enumerate(summary_features):\n",
    "    if i==0: \n",
    "        l = 'brief'\n",
    "    else: \n",
    "        l=False\n",
    "    \n",
    "    a1=i%4\n",
    "    a2 = int(i/4)\n",
    "    sns.lineplot(x='year',\n",
    "                 y=each,\n",
    "                 data=df_final_plots[df_final_plots.year!=1950],\n",
    "                 hue='fha_grade',\n",
    "                 hue_order=['A','B','C','D'],\n",
    "                 palette={\"A\":\"#5FCE72\",\"B\":\"#0BC0ED\",\"C\":\"#FFD419\",\"D\":\"#FF4B19\"},\n",
    "                 legend=l,err_style='bars',\n",
    "                 ax=axes[a1,a2],estimator=np.mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Pre-Treatment Choropleth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(20,8))\n",
    "\n",
    "df_final_avg1[df_final_avg1.period=='pre'].plot('ln_median_homevalue_adj',cmap='viridis_r', ax=ax[0], legend=True, linewidth=0.1,scheme='quantiles')\n",
    "df_final_avg1[df_final_avg1.period=='pre'].plot('owned_perc',cmap='viridis_r', ax=ax[1], legend=True, linewidth=0.1,scheme='quantiles')\n",
    "df_final_avg1[df_final_avg1.period=='pre'].plot('e_w',cmap='viridis_r', ax=ax[2], legend=True, linewidth=0.1,scheme='quantiles')\n",
    "\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "\n",
    "ax[0].set_title('Pre-Treatment Log Median Home Value')\n",
    "ax[1].set_title('Pre-Treatment Ownership Percentage')\n",
    "ax[2].set_title('Pre-Treatment Spatially Weighted Entropy Index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 FHA/HOLC Comparison\n",
    "- A count of which tracts are labeled the same vs different. \n",
    "- When different, how is it different? \n",
    "- Side by side map comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fha_holc_test = df_final2[(df_final2.period=='pre') &\n",
    "                           (df_final2.holc_grade.isin(['A','B','C','D']))][['fha_grade','holc_grade','gisjoin_1940']]\n",
    "df_fha_holc_test['holc_grade_new'] = df_fha_holc_test['holc_grade'].apply(lambda x: x[0])\n",
    "\n",
    "df_fha_holc_test['same'] = df_fha_holc_test.apply(lambda x: 'y' if x['holc_grade']==x['fha_grade'] else 'n',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the FHA and HOLC labels are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fha_holc_test.groupby('same').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the two maps are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(16,10))\n",
    "    \n",
    "for label, data in df_final2[(df_final2.year==1940)\\\n",
    "                             &(df_final2.fha_grade.isna()==False)].groupby('fha_grade'): \n",
    "    color = paper_cmap[label]\n",
    "    data = gpd.GeoDataFrame(data,geometry='geometry',crs=4326).to_crs(epsg=3857)\n",
    "    data.to_crs(epsg=3857).plot(color=color,ax=ax1, label=label, legend=False, linewidth=0.1,)\n",
    "    \n",
    "for label, data in df_final2[(df_final2.year==1940)\\\n",
    "                             &(df_final2.holc_grade.isna()==False)].groupby('holc_grade'): \n",
    "    color = paper_cmap[label]\n",
    "    data = gpd.GeoDataFrame(data,geometry='geometry',crs=4326).to_crs(epsg=3857)\n",
    "    data.plot(color=color,ax=ax2, label=label, legend=True, linewidth=0.1)\n",
    "    \n",
    "df_final2[(df_final2.year==1940)\\\n",
    "          &(df_final2.fha_grade.isna()==False)\\\n",
    "          &(df_final2.holc_grade!=df_final2.fha_grade)].to_crs(epsg=3857).plot(ax=ax1, facecolor=\"none\", \n",
    "              edgecolor=\"gray\", lw=0.7,linestyle='--')\n",
    "df_final2[(df_final2.year==1940)\\\n",
    "          &(df_final2.fha_grade.isna()==False)\\\n",
    "          &(df_final2.fha_grade!=df_final2.holc_grade)].to_crs(epsg=3857).plot(ax=ax2, facecolor=\"none\", \n",
    "              edgecolor=\"gray\", lw=0.7,linestyle='--')\n",
    "\n",
    "  \n",
    "ax2.set_title('Tract Labels by HOLC Map')\n",
    "ax1.set_title('Tract Labels by FHA Map')\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Propensity Score Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features to run the propensity scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pscore_features= ['white_perc','white_perc_chng','black_perc','black_perc_chng',\n",
    "                'population_density','population_density_chng_perc',\n",
    "                'population','population_chng_perc',\n",
    "                'college_perc','ln_median_homevalue_adj','ln_median_homevalue_adj_chng_perc',\n",
    "                'owned_perc','owned_perc_chng','perc_college_10plus','perc_black_20plus','dist_downtown','e_w','e_w_chng_perc']\n",
    "\n",
    "pscore_features_chng = ['ln_median_homevalue_adj_chng_perc',\n",
    "                  'owned_perc_chng','e_w_chng_perc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm['fha_grade_psm'] = df_final_psm.apply(lambda x: 'D' if x['fha_grade']=='D' else 'not_D',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Estimate weights\n",
    "Weights will probably be slightly different each time and resulting tracts that remain also different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "### Process the data\n",
    "le = preprocessing.LabelEncoder()\n",
    "scaler =preprocessing.MinMaxScaler()\n",
    "std_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "### Fill missing values with the column mean\n",
    "X = df_final_psm[pscore_features]\n",
    "X = X.fillna(value=X.mean(axis=0)) \n",
    "\n",
    "##### Transform all to 0-1 scale.\n",
    "X1 = pd.DataFrame(data=std_scaler.fit_transform(X,),columns=X.columns, index=X.index)\n",
    "\n",
    "X1 = pd.merge(df_final_psm[['fha_grade_psm']],\n",
    "              X1,left_index=True,right_index=True)\n",
    "X1 = X1[pscore_features]\n",
    "\n",
    "y = le.fit_transform(df_final_psm['fha_grade_psm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "rf= RandomForestClassifier()\n",
    "rf1 = CalibratedClassifierCV(rf, cv=5,method='isotonic')\n",
    "\n",
    "\n",
    "rf1 = rf1.fit(X1,y)\n",
    "rf = rf.fit(X1,y)\n",
    "\n",
    "print(\"Mean acccuracy of RF {:.3f}\".format(rf1.score(X1, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RF\n",
    "y_probs = cross_val_predict(rf1, X1, y,cv=5,method='predict_proba')\n",
    "df_final_psm['{}_prob'.format(le.classes_[0])]= y_probs[:,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X1.columns,\n",
    "                                    columns=['importance']).sort_values('importance',  ascending=False)\n",
    "\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Turn probabilities into propensity weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm_cd = df_final_psm[df_final_psm.fha_grade.isin(['C','D'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm.loc[df_final_psm['D_prob']==1,'D_prob']=.999999\n",
    "df_final_psm.loc[df_final_psm['D_prob']==0,'D_prob']=.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_name='weights3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_psm[weights_name] = df_final_psm.apply(lambda x: 1/x['D_prob'] if x['fha_grade_psm']=='D' else 1/(1-x['D_prob']),axis=1)\n",
    "df_final_psm[weights_name] = df_final_psm[weights_name].apply(lambda x: 100 if x>100 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Filter years and PSM \n",
    "\n",
    "Filter for years that are not 1950 and also here's the place to filter for more stringent conditions such as \n",
    "- Tracts with non-black neighbors\n",
    "- D-probs below a certain treshold. (using 25% here)\n",
    "- Also find the non-D tracts that have a high prob of being D and were not??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Filter for probabilties**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### create a DF with just the tract pairs\n",
    "def final_df(weights_col,d_t=.9):\n",
    "    D_thres = d_t\n",
    "\n",
    "    df_allyrs= df_final_avg1[\n",
    "                ((df_final_avg1['gisjoin'].isin(df_final_psm[(df_final_psm['D_prob']<=D_thres)\\\n",
    "                                                             &(df_final_psm['fha_grade_psm']=='D')].gisjoin.values))\n",
    "                 | \\\n",
    "                 (df_final_avg1['gisjoin'].isin(df_final_psm[(df_final_psm['D_prob']>=(1-D_thres))\\\n",
    "                                                             &(df_final_psm['fha_grade_psm']!='D')].gisjoin.values))\n",
    "                )\n",
    "                     &\\\n",
    "                (df_final_avg1['gisjoin'].isin(df_final_psm[df_final_psm[weights_col]<=100].gisjoin.values))\n",
    "               ]\n",
    "\n",
    "    df_allyrs_norm=pd.merge(df_allyrs,df_final_psm[['gisjoin',weights_col,'PRI_NEIGH']],how='left',on='gisjoin')\n",
    "    return(df_allyrs_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_norm = final_df(weights_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_norm_holc_paper_aug7 = gpd.read_file('../results/df_allyrs_norm_holc_paper_aug7.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12, 6))\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "chicago_boundary = gpd.read_file('data/chicago_boundary/geo_export_f05488dc-4f9d-49be-81e3-c094992d4c80.shp')\n",
    "\n",
    "chicago_boundary.plot(ax=ax[0],color='white',edgecolor='#454545',linewidth =.4,)\n",
    "chicago_boundary.plot(ax=ax[1],color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "for label, data in df_allyrs_norm[df_allyrs_norm.period=='pre'].groupby('fha_grade'): \n",
    "    color = paper_cmap[label]\n",
    "    data.plot(color=color,ax=ax[0], label=label, legend=True,linewidth=0.1)\n",
    "    \n",
    "for label, data in df_allyrs_norm_holc_paper_aug7[df_allyrs_norm_holc_paper_aug7.period=='pre'].groupby('holc_grade'): \n",
    "    color = paper_cmap[label]\n",
    "    data.plot(color=color,ax=ax[1], label=label,linewidth=0.1)\n",
    "\n",
    "\n",
    "ax[0].set_title('D-nD FHA tracts {} threshold'.format(0.9))\n",
    "ax[1].set_title('D-nD HOLC tracts {} threshold'.format(0.9))\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[0].legend()\n",
    "\n",
    "# plt.savefig('results/fha_holc_sidebyside_map.png',bbox_inches = 'tight',\n",
    "#     pad_inches = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compare D against non-D and C, using weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavg(group, avg_name, weight_name):\n",
    "    \n",
    "    \"\"\" http://stackoverflow.com/questions/10951341/pandas-dataframe-aggregate-function-using-multiple-columns\n",
    "    In rare instance, we may not have weights, so just return the mean. Customize this if your business case\n",
    "    should return otherwise.\n",
    "    \"\"\"\n",
    "    d = group[avg_name]\n",
    "    w = group[weight_name]\n",
    "    try:\n",
    "        return (d * w).sum() / w.sum()\n",
    "#         return d.mean()\n",
    "    except ZeroDivisionError:\n",
    "        return d.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the two groups look like now once we've adjusted for the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4.1 Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pscore_features_chng = ['ln_median_homevalue_adj_chng_perc', 'owned_perc_chng', 'e_w_chng_perc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "d_nd_dist = []\n",
    "d_c_dist = []\n",
    "\n",
    "thres_range = np.arange(0.1,1,.01)\n",
    "\n",
    "for i in thres_range:\n",
    "    df_allyrs_norm = final_df(weights_name,d_t=i)\n",
    "\n",
    "    df_wavg = pd.DataFrame(columns = pscore_features_chng,index= ['D', 'not_D_fha'])\n",
    "    for each in pscore_features_chng: \n",
    "        df_wavg.loc[:,each] = df_allyrs_norm[df_allyrs_norm.period=='pre'].groupby(\"D_fha\").apply(wavg, each,weights_name).values\n",
    "        \n",
    "        \n",
    "    df_wavg = df_wavg.transpose()\n",
    "    \n",
    "    d_nd_dist.append(scipy.spatial.distance.pdist(df_wavg.transpose())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thres_range,d_nd_dist,label='D-nD')\n",
    "plt.title('Sum of differences between two FHA groups')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres =.9\n",
    "\n",
    "df_allyrs_norm = final_df(weights_name,d_t=thres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Compare FHA and HOLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_norm_holc_paper_aug7 = gpd.read_file('../results/df_allyrs_norm_holc_paper_aug7.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df_allyrs_norm[(df_allyrs_norm.period=='pre') \\\n",
    "                   &(df_allyrs_norm.holc_grade.isin(['A','B','C','D']))][['fha_grade','holc_grade','gisjoin','geometry',weights_name]]\n",
    "\n",
    "df_fha_holc_subgroup = pd.merge(t,df_final_psm[['gisjoin','D_prob']],on='gisjoin')\n",
    "df_fha_holc_subgroup['holc_grade_new'] = df_fha_holc_subgroup['holc_grade'].apply(lambda x: x[0])\n",
    "df_fha_holc_subgroup['same'] = df_fha_holc_subgroup.apply(lambda x: 'y' if x['holc_grade']==x['fha_grade'] else 'n',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holc_propensity_weights = df_allyrs_norm_holc_paper_aug7[df_allyrs_norm_holc_paper_aug7.period=='pre'].rename(columns={'D_prob':'holc_D_prob'})[['gisjoin','PRI_NEIGH','holc_D_prob']],\n",
    "df_fha_holc_subgroup = pd.merge(df_fha_holc_subgroup,\n",
    "                                holc_propensity_weights,\n",
    "                                on='gisjoin',\n",
    "                                how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fha_holc_subgroup.groupby(['same','fha_grade','holc_grade']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare FHA and HOLC Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12, 6))\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "chicago_boundary = gpd.read_file('data/chicago_boundary/geo_export_f05488dc-4f9d-49be-81e3-c094992d4c80.shp')\n",
    "\n",
    "chicago_boundary.plot(ax=ax[0],color='white',edgecolor='#454545',linewidth =.4,)\n",
    "chicago_boundary.plot(ax=ax[1],color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "\n",
    "pd.merge(df_allyrs_norm[(df_allyrs_norm.period=='pre')&(df_allyrs_norm.weights3<100)],df_final_psm[['gisjoin','D_prob']],on='gisjoin').plot('weights3',\n",
    "                                                                                                              legend=True,\n",
    "                                                                                                              ax=ax[0],\n",
    "                                                                                                             cmap='GnBu',\n",
    "                                                                                                              scheme='quantiles',\n",
    "                                                                                                              linewidth=0.1)\n",
    "\n",
    "df_allyrs_norm_holc_paper_aug7.plot('weights8',legend=True,ax=ax[1],cmap='GnBu',scheme='quantiles',linewidth=0.1)\n",
    "\n",
    "\n",
    "\n",
    "ax[0].set_title('FHA Weights'.format(0.9))\n",
    "ax[1].set_title('HOLC Weights'.format(0.9))\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at low probabilty D Tracts - FHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "chicago_boundary = gpd.read_file('data/chicago_boundary/geo_export_f05488dc-4f9d-49be-81e3-c094992d4c80.shp')\n",
    "\n",
    "chicago_boundary.plot(ax=ax,color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "df_fha_lowprobD = pd.merge(df_allyrs_norm[(df_allyrs_norm.period=='pre')&\\\n",
    "                        (df_allyrs_norm.weights3<100)&\\\n",
    "                        (df_allyrs_norm.fha_grade=='D')],df_final_psm[['gisjoin','D_prob']],on='gisjoin')\n",
    "\n",
    "df_fha_lowprobD=df_fha_lowprobD[df_fha_lowprobD['D_prob']<.5]\n",
    "\n",
    "neigh_fiveormorecount = df_fha_lowprobD.groupby('PRI_NEIGH').count().sort_values('gisjoin',ascending=False).reset_index().reset_index()\n",
    "\n",
    "## Plot Neighborhoods that include these tracts\n",
    "neighborhoods_example = neighorhoods[neighorhoods.PRI_NEIGH.isin(neigh_fiveormorecount.PRI_NEIGH)].to_crs({'init':'epsg:4326'})\n",
    "neighborhoods_example['coords'] = neighborhoods_example['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "neighborhoods_example['coords'] = [coords[0] for coords in neighborhoods_example['coords']]\n",
    "for idx, row in neighborhoods_example.iterrows():\n",
    "    plt.annotate(s=row['PRI_NEIGH'], xy=row['coords'],\n",
    "                 horizontalalignment='center',)\n",
    "    \n",
    "neighborhoods_example.plot(ax=ax,color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "\n",
    "df_fha_lowprobD.plot('D_prob',\n",
    "                      legend=True,\n",
    "                      ax=ax,\n",
    "                     cmap='viridis',\n",
    "                      scheme='Quantiles',k=5,\n",
    "                      linewidth=0.1)\n",
    "\n",
    "\n",
    "ax.set_title('Probability of D label - FHA D Tracts'.format(0.9))\n",
    "ax.set_axis_off()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracts with different FHA and HOLC Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(12, 6))\n",
    "plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "            hspace = 0, wspace = 0)\n",
    "chicago_boundary = gpd.read_file('data/chicago_boundary/geo_export_f05488dc-4f9d-49be-81e3-c094992d4c80.shp')\n",
    "\n",
    "chicago_boundary.plot(ax=ax,color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "\n",
    "df_fha_diff=df_fha_holc_test_subgroup_paper[(df_fha_holc_test_subgroup_paper.same=='n')&\\\n",
    "                                (df_fha_holc_test_subgroup_paper.fha_grade=='D')]\n",
    "\n",
    "\n",
    "## Plot Neighborhoods that include these tracts\n",
    "neighborhoods_example = neighorhoods[neighorhoods.PRI_NEIGH.isin(df_fha_diff.PRI_NEIGH)].to_crs({'init':'epsg:4326'})\n",
    "neighborhoods_example['coords'] = neighborhoods_example['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "neighborhoods_example['coords'] = [coords[0] for coords in neighborhoods_example['coords']]\n",
    "for idx, row in neighborhoods_example.iterrows():\n",
    "    plt.annotate(s=row['PRI_NEIGH'], xy=row['coords'],\n",
    "                 horizontalalignment='center')\n",
    "    \n",
    "neighborhoods_example.plot(ax=ax,color='white',edgecolor='#454545',linewidth =.4,)\n",
    "\n",
    "\n",
    "df_fha_diff.plot('D_prob',\n",
    "                      legend=True,\n",
    "                      ax=ax,\n",
    "                     cmap='viridis',\n",
    "                      scheme='Quantiles',k=5,\n",
    "                      linewidth=0.1)\n",
    "\n",
    "\n",
    "ax.set_title('Probability of D label - FHA D Tracts not graded D by HOLC'.format(0.9))\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. DiD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "df_final_psm[df_final_psm.fha_grade_psm=='D']['{}_prob'.format(le.classes_[0])].hist(bins=30,density=True,alpha=.5,label='D')\n",
    "df_final_psm[df_final_psm.fha_grade_psm=='not_D']['{}_prob'.format(le.classes_[0])].hist(bins=30,density=True,alpha=.5,label='non-D')\n",
    "\n",
    "plt.vlines(thres,0,16,linestyles='dashed',color='C0',lw=2.5,alpha=0.5,label='D threshold')\n",
    "plt.vlines(1-thres,0,16,linestyles='dashed',color='C1',lw=2.5,alpha=0.5,label='non-D threshold')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Tracts Used Given Probability of D FHA Grade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wavg = pd.DataFrame(columns = pscore_features_chng,index= ['D', 'not_D_fha'])\n",
    "\n",
    "for each in pscore_features_chng: \n",
    "    df_wavg.loc[:,each] = df_allyrs_norm[df_allyrs_norm.period=='pre'].groupby(\"D_fha\").apply(wavg, each,weights_name).values\n",
    "\n",
    "print(df_wavg.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DiD Model home values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create dummies for different periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_period(x):\n",
    "    if x <=1940: \n",
    "        return 'pre'\n",
    "    elif x<=1980: \n",
    "        return 'post'\n",
    "    else: \n",
    "        return 'reversal'\n",
    "\n",
    "    \n",
    "def get_dummies(df):\n",
    "\n",
    "    period_dummy = pd.get_dummies(df['period'].astype(str),prefix='period')\n",
    "    period_dummy = period_dummy[['period_pre','period_post','period_reversal']]\n",
    "\n",
    "    grade_dummy = pd.get_dummies(df['fha_grade'],prefix=None)\n",
    "    grade_dummy = pd.DataFrame(grade_dummy,index=df.index).rename(columns={'fha_grade':'grade_dummy'})\n",
    "\n",
    "    grade_dummy = df['fha_grade'].apply(lambda x: 1 if x=='D' else 0)\n",
    "    grade_dummy = pd.DataFrame(grade_dummy,index=df.index).rename(columns={'fha_grade':'D'})\n",
    "\n",
    "\n",
    "    neigh_dummy = pd.get_dummies(df['PRI_NEIGH'].astype(str),prefix=None)\n",
    "    neigh_dummy = pd.DataFrame(neigh_dummy).rename(columns={'PRI_NEIGH':'n_fe'})\n",
    "    neigh_dummy= neigh_dummy.iloc[:,1:]\n",
    "    \n",
    "    tract_dummy = pd.get_dummies(df['gisjoin'].astype(str),prefix=None)\n",
    "    tract_dummy = pd.DataFrame(tract_dummy).rename(columns={'gisjoin':'tract_fe'})\n",
    "    tract_dummy= tract_dummy.iloc[:,1:]\n",
    "\n",
    "\n",
    "    dummies_per = pd.concat([period_dummy, grade_dummy],axis=1)\n",
    "    \n",
    "    return df,period_dummy,grade_dummy,tract_dummy,neigh_dummy,dummies_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_dummies( period_dummy,dummies_per,grade_dummy):\n",
    "    string1 = \"\"    \n",
    "    \n",
    "    \n",
    "    for j in grade_dummy.columns: \n",
    "        for i in pd.concat([period_dummy],axis=1).columns: \n",
    "            string1= string1+ (\"{}:{}+\".format(i,j))\n",
    "    \n",
    "    per_grade_dummy = PatsyTransformer(string1[:-1], return_type=\"dataframe\").fit_transform(dummies_per) ## Clipping off the last bit of the '+'\n",
    "\n",
    "    return per_grade_dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_interaction_dummies( period_dummy1,features):\n",
    "    string1 = \"\"\n",
    "    \n",
    "    features_per = pd.concat([period_dummy1,features],axis=1)\n",
    "\n",
    "    \n",
    "    for j in features.columns: \n",
    "        for i in pd.concat([period_dummy1],axis=1).columns: \n",
    "            string1= string1+ (\"{}:{}+\".format(i,j))\n",
    "    \n",
    "    per_feature_dummy = PatsyTransformer(string1[:-1], return_type=\"dataframe\").fit_transform(features_per) ## Clipping off the last bit of the '+'\n",
    "\n",
    "    return per_feature_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Create regression summary functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for running regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_reg(df,weights_col,features,outcome,g_dum,p_dum,p_g_dum,n_dum,model='WLS',std=False, lag_x=False,lag_y=False,neigh=False):\n",
    "    import datetime\n",
    "    now = datetime.date.today()\n",
    "    \n",
    "    X1 = df[features]\n",
    "    y1_1 = df[outcome]\n",
    "\n",
    "    ### Adjust for spatial autocorrelation\n",
    "    if lag_x==True: \n",
    "        X1_l = get_lags(df,features)\n",
    "    if lag_y==True: \n",
    "        y1_l = get_lags(df,[y])\n",
    "\n",
    "    ### Standardize optional\n",
    "    if std==True: \n",
    "        X1 = (X1-X1.mean())/X1.std()\n",
    "        y1_1 = (y1_1-y1_1.mean())/y1_1.std()\n",
    "        if lag_x==True:\n",
    "            X1_l = (X1_l-X1_l.mean())/X1_l.std()\n",
    "        if lag_y==True: \n",
    "            y1_l = (y1_l-y1_l.mean())/y1_l.std()\n",
    "    covariates = [X1,g_dum,p_dum,p_g_dum]\n",
    "    \n",
    "    if lag_x==True: \n",
    "        covariates.append(X1_l)\n",
    "    if lag_y==True: \n",
    "        covariates.append(y1_l)\n",
    "    if neigh==True: \n",
    "        covariates.append(n_dum)\n",
    "        \n",
    "        \n",
    "    X1_1= pd.concat(covariates,ignore_index=False,axis=1)\n",
    "    X1_1 = X1_1.fillna(value=X1_1.mean(axis=0))\n",
    "\n",
    "    weights = df[weights_col]\n",
    "    ####################\n",
    "    ##       OLS      ##\n",
    "    ####################\n",
    "    if model=='OLS':\n",
    "        mdl = sm.OLS(y1_1,X1_1,hasconst=True)\n",
    "        results = mdl.fit()\n",
    "\n",
    "\n",
    "        df['resid'] = results.resid\n",
    "        df['y'] = y1_1\n",
    "        df['y_pred'] = results.predict()\n",
    "\n",
    "        print(results.summary())\n",
    "        \n",
    "        \n",
    "    ####################\n",
    "    ##       WLS      ##\n",
    "    ####################\n",
    "    if model=='WLS':\n",
    "        mdl = sm.WLS(y1_1,X1_1,weights=weights,hasconst=False)\n",
    "        results = mdl.fit()\n",
    "\n",
    "\n",
    "        df['resid'] = results.resid\n",
    "        df['y'] = y1_1\n",
    "        df['y_pred'] = results.predict()\n",
    "\n",
    "        print(results.summary())\n",
    "\n",
    "\n",
    "    ####################\n",
    "    ## Applies to all ##\n",
    "    ####################\n",
    "\n",
    "    # Display the autocorrelation plot of your time series\n",
    "    from statsmodels.graphics import tsaplots\n",
    "\n",
    "    fig = tsaplots.plot_acf(df['resid'], lags=2)\n",
    "    plt.show()\n",
    "\n",
    "    for i in ['resid']:\n",
    "        print(i)\n",
    "        for y in df.period.unique():\n",
    "            W = ps.weights.Queen.from_dataframe(gpd.GeoDataFrame(df[df.period==y],\n",
    "                                                                           geometry='geometry',\n",
    "                                                                           crs={'init':'epsg:4326'}).to_crs(epsg=3857),\n",
    "                                               geom_col='geometry',\n",
    "                                               ids='gisjoin')\n",
    "            W.transform='r'\n",
    "            mi = Moran(df[df.period==y][i].values, W, two_tailed=True)\n",
    "            print(\"%.3f\"%mi.I,\"%.5f\"%mi.p_norm, \"%.3f\"%mi.EI)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Function for Creating Spatial Lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lags(df,features):\n",
    "    Xl_lst = []\n",
    "    for y in df.period.unique():\n",
    "        W = ps.weights.Queen.from_dataframe(gpd.GeoDataFrame(df[df.period==y],\n",
    "                                                                       geometry='geometry',\n",
    "                                                                       crs={'init':'epsg:4326'}).to_crs(epsg=3857),\n",
    "                                           geom_col='geometry',\n",
    "                                           ids='gisjoin')\n",
    "\n",
    "        W.transform='r'\n",
    "        Xl = ps.lag_spatial(W,df[df.period==y][features])\n",
    "        Xl_lst.append(Xl)\n",
    "    Xl_stack=np.vstack(tuple(Xl_lst))\n",
    "\n",
    "    X_lag = pd.DataFrame(Xl_stack,\n",
    "                 columns=['{}_l'.format(x) for x in features],\n",
    "                 index = df.index)\n",
    "    return X_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_norm_subgroup,period_dummy1,grade_dummy,tract_dummy,neigh_dummy,dummies_per = get_dummies(df_allyrs_norm)\n",
    "period_dummy =period_dummy1[['period_post','period_reversal']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_grade_dummy = get_interaction_dummies(period_dummy,dummies_per,grade_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Model 1: Home Values\n",
    "- Just 1930 and 1940 values in the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_name = 'weights3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1_features= ['white_perc','white_perc_chng','black_perc','black_perc_chng',\n",
    "                'population_density','population_density_chng_perc',\n",
    "                'population','population_chng_perc',\n",
    "                'college_perc',\n",
    "                'owned_perc','owned_perc_chng','perc_college_10plus','perc_black_20plus','dist_downtown','e_w']\n",
    "\n",
    "# mdl1_features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_mdl1_1 = pd.merge(df_allyrs_norm[['per_order','period','gisjoin']+['ln_median_homevalue_adj','fha_grade','D_fha',weights_name]],\n",
    "                                 df_final_psm[['gisjoin','geometry']+mdl1_features],\n",
    "                                 on=['gisjoin'],how='left')\n",
    "\n",
    "\n",
    "feat_dummies = get_feature_interaction_dummies(period_dummy1,df_allyrs_mdl1_1[mdl1_features])\n",
    "df_allyrs_mdl1_1 = pd.concat([df_allyrs_mdl1_1,feat_dummies],axis=1)\n",
    "mld1_1_results = run_reg(df_allyrs_mdl1_1,weights_name,feat_dummies.columns,'ln_median_homevalue_adj',\n",
    "                         grade_dummy,period_dummy,per_grade_dummy,neigh_dummy,\n",
    "                         model='WLS',std=True, lag_x=False,lag_y=False,neigh=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Model 2: Home Ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl2_features= ['white_perc','white_perc_chng','black_perc','black_perc_chng',\n",
    "                'population_density','population_density_chng_perc',\n",
    "                'population','population_chng_perc',\n",
    "                'college_perc',\n",
    "                'ln_median_homevalue_adj','ln_median_homevalue_adj_chng_perc',\n",
    "                'perc_college_10plus','perc_black_20plus','dist_downtown','e_w','e_w_chng_perc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allyrs_mdl2_1 = pd.merge(df_allyrs_norm[['per_order','period','gisjoin']+['owned_perc','fha_grade','D_fha',weights_name]],\n",
    "                                 df_final_psm[['gisjoin','geometry']+mdl2_features],\n",
    "                                 on=['gisjoin'],how='left')\n",
    "\n",
    "\n",
    "feat_dummies = get_feature_interaction_dummies(period_dummy1,df_allyrs_mdl2_1[mdl2_features])\n",
    "\n",
    "df_allyrs_mdl2_1 = pd.concat([df_allyrs_mdl2_1,feat_dummies],axis=1)\n",
    "\n",
    "mld2_1_results = run_reg(df_allyrs_mdl2_1,weights_name,feat_dummies.columns,'owned_perc',\n",
    "                         grade_dummy,period_dummy,per_grade_dummy,neigh_dummy,\n",
    "                         model='WLS',std=True, lag_x=False,lag_y=False,neigh=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Model 3: Segregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3_features= ['owned_perc','owned_perc_chng',\n",
    "                'population_density','population_density_chng_perc',\n",
    "                'population','population_chng_perc',\n",
    "                'college_perc',\n",
    "                'ln_median_homevalue_adj','ln_median_homevalue_adj_chng_perc','perc_college_10plus','dist_downtown']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3.1: Not year-standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_allyrs_mdl3_1 = pd.merge(df_allyrs_norm_subgroup[['per_order','period','gisjoin']+['black_perc','e_w','fha_grade','D_fha',weights_name]],\n",
    "                                 df_final_psm[['gisjoin','geometry']+mdl3_features],\n",
    "                                 on=['gisjoin'],how='left')\n",
    "feat_dummies = get_feature_interaction_dummies(period_dummy1,df_allyrs_mdl3_1[mdl3_features])\n",
    "\n",
    "df_allyrs_mdl3_1 = pd.concat([df_allyrs_mdl3_1,feat_dummies],axis=1)\n",
    "\n",
    "mld3_1_results = run_reg(df_allyrs_mdl3_1,weights_name,feat_dummies.columns,'e_w',\n",
    "                         grade_dummy,period_dummy,per_grade_dummy,neigh_dummy,model='WLS',\n",
    "                         std=True, lag_x=False,lag_y=False,neigh=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
